{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Web Scraping with Python: An Introductory Tutorial\n",
    "\n",
    "### By Rob Osterburg, RRCC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics\n",
    "* Using requests and beautifulsoup to gather data from web pages\n",
    "* Go over key concepts, tools and techniques as we go\n",
    "* Scrape some data \n",
    "* Organize it using namedtuple\n",
    "* Persist it as a JSON file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Key Packages\n",
    "\n",
    "* requests -- Issues HTTP requests to web servers and handles the response \n",
    "\n",
    "* beautifulsoup4 -- Creates a searchable tree from web page\n",
    "\n",
    "* These packages are *not* in Python's standard library, here is how to install them:\n",
    " \n",
    "    - Standard Python: `pip install requests beautifulsoup4`\n",
    "\n",
    "    - Anaconda Python: `conda install requests beautifulsoup4`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Requests is 'HTTP for Humans'\n",
    "import requests\n",
    "\n",
    "# BeautifulSoup parses and builds tree from HTML\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Warning! Warning!\n",
    "* Web scraping is commonly frowned upon by the site's owners\n",
    "* **Always check** \n",
    "    - **Terms of service**  \n",
    "    - **Conditions of use**\n",
    "    - **robots.txt file**   \n",
    "* Aggressive scrapers can take a site down\n",
    "* If in doubt, talk to a lawyer, especially for *anything* work related\n",
    "* Related information and experiences: [Sustainable Scrapers, PyData DC, 2016](http://pyvideo.org/pydata-dc-2016/sustainable-scrapers.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "* Very real legal issues that can arise from web scraping \n",
    "* Taking a site down is a *denial of service attack* covered by the US Computer Fraud and Abuse Act \n",
    "* What about PyVideo.org?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## PyVideo.org - Terms and Conditions?\n",
    "\n",
    "None that I can see, the site's [About page](http://pyvideo.org/pages/about.html) says:\n",
    "\n",
    "> ... PyVideo.org is a freely available index of freely available resources that seek to provide everyone with the opportunity to learn about Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## PyVideo.org - Robots.txt?\n",
    "* A robots.txt files contain *restrictions* on the content that web crawlers are permitted to access \n",
    "\n",
    "* The [http://pyvideo.org/robots.txt](http://pyvideo.org/robots.txt) is empty, except for one comment \n",
    "\n",
    "* It appears that there are no restrictions on scraping PyVideo.org  \n",
    "\n",
    "* Let's minimize our impact on the site \n",
    "\n",
    "* To see *The Robots Exclusion Protocol* visit [http://www.robotstxt.org/](http://www.robotstxt.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting the Page\n",
    "resp = requests.get('http://pyvideo.org/tags.html')\n",
    "resp.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## HTTP Responses\n",
    "\n",
    "HTTP Verb | Effect | Success | Failure\n",
    "--------- | ------ | ------- | --------\n",
    "POST      | Create | 200     | 400, 40X, 500\n",
    "**GET**       | **Read**   | **200**     | **400, 40X, 500**\n",
    "PUT       | Update | 200     | 400, 40X, 500 \n",
    "DELETE    | Delete | 200     | 400, 40X, 500\n",
    "\n",
    "* [Comprehensive list of HTTP status codes](https://httpstatuses.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "* GET is all we will need today\n",
    "* Status != 200 is a problem of some sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Parse the Page\n",
    "soup = BeautifulSoup(resp.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " ## BeautifulSoup - Parses the page\n",
    " \n",
    "* Models the page as a tree\n",
    "* Similar to the Document Object Model\n",
    " ![tags page structure](./images/htmltree.png)\n",
    "\n",
    "* Parsers build the tree, pick one\n",
    " - html.parser -> Default parser for Python 3\n",
    " - HTMLParser  -> Default parser for Python 2\n",
    " - lxml        -> Fast requires installing a C library and a PyPI package \n",
    " - html5lib    -> Pure Python and part of the standard library\n",
    " \n",
    " cite: https://interactivepython.org/runestone/static/pythonds/Trees/ExamplesofTrees.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## BeautifulSoup - Well documented and easy to use API\n",
    "* select('css selector') --> [List of Tags]\n",
    "\n",
    "* find_all(tags, keyword_args, attrs={'attr', 'value'})  --> [List of Tags]\n",
    "    \n",
    "* find(tags, keyword_args, attrs={'attr', 'value'}) --> Tag\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Understanding Page Structure\n",
    "\n",
    "* To extract data from a page, you need to understand its structure\n",
    "* BeautifulSoup holds the tree in memory\n",
    "* Look into it using the our browser's developer tools\n",
    "    - Mac -- Cmd + Opt + i\n",
    "    - Linux -- Ctrl + Shift + i\n",
    "    - Windows -- Ctrl + Shift + i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Visiting the [Tags page](http://pyvideo.org/tags) page, you can see the structure of the page  \n",
    "![tags page structure](./images/structure-of-tags-page.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://pyvideo.org/index.html',\n",
       " 'http://pyvideo.org/events.html',\n",
       " 'http://pyvideo.org/tags.html',\n",
       " 'http://pyvideo.org/speakers.html',\n",
       " 'http://pyvideo.org/pages/about.html',\n",
       " 'http://pyvideo.org/pages/thank-you-contributors.html',\n",
       " 'http://pyvideo.org/pages/thanks-will-and-sheila.html',\n",
       " 'http://pyvideo.org/tag/2to3/',\n",
       " 'http://pyvideo.org/tag/3d/',\n",
       " 'http://pyvideo.org/tag/3d-printing/']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Building a list of links using a CSS selector and a for loop \n",
    "topics = []\n",
    "for topic in soup.select('li > a'):\n",
    "    # extract the link using the href attribute\n",
    "    topics.append(topic['href'])\n",
    "topics[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a href=\"http://pyvideo.org/index.html\"><i class=\"fa fa-fw fa-home\"></i> <span>Start</span></a>,\n",
       " <a href=\"http://pyvideo.org/events.html\"><i class=\"fa fa-fw fa-list-ul\"></i> <span>Events</span></a>,\n",
       " <a href=\"http://pyvideo.org/tags.html\"><i class=\"fa fa-fw fa-tags\"></i> <span>Tags</span></a>,\n",
       " <a href=\"http://pyvideo.org/speakers.html\"><i class=\"fa fa-fw fa-users\"></i> <span>Speakers</span></a>,\n",
       " <a href=\"http://pyvideo.org/pages/about.html\"><i class=\"fa fa-fw fa-info\"></i> <span>About</span></a>,\n",
       " <a href=\"http://pyvideo.org/pages/thank-you-contributors.html\"><i class=\"fa fa-fw fa-info\"></i> <span>Thank You</span></a>,\n",
       " <a href=\"http://pyvideo.org/pages/thanks-will-and-sheila.html\"><i class=\"fa fa-fw fa-info\"></i> <span></span></a>,\n",
       " <a href=\"http://pyvideo.org/tag/2to3/\">2to3</a>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Building a list of links using a CSS selector and a list comprehension\n",
    "topics = [topic \n",
    "          for topic in soup.select('li > a')]\n",
    "topics[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<a href=\"http://pyvideo.org/tag/2to3/\">2to3</a>,\n",
       " bs4.element.Tag,\n",
       " ['2to3'],\n",
       " 'http://pyvideo.org/tag/2to3/')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic = topics[7]\n",
    "topic, type(topic), topic.contents, topic['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1461,\n",
       " bs4.element.Tag,\n",
       " [<a href=\"http://pyvideo.org/tag/2to3/\">2to3</a>,\n",
       "  <a href=\"http://pyvideo.org/tag/3d/\">3d</a>,\n",
       "  <a href=\"http://pyvideo.org/tag/3d-printing/\">3D Printing</a>,\n",
       "  <a href=\"http://pyvideo.org/tag/abc/\">abc</a>,\n",
       "  <a href=\"http://pyvideo.org/tag/accelerate/\">accelerate</a>])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clean up links\n",
    "topics = [topic \n",
    "          for topic in topics \n",
    "          if r'tag/' in topic['href']]\n",
    "len(topics), type(topics[0]), topics[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's visit the topic page\n",
    "resp = requests.get('http://pyvideo.org/tag/scraping')\n",
    "resp.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Parse the page\n",
    "soup = BeautifulSoup(resp.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://pyvideo.org/pydata-dc-2016/open-data-dashboards-python-web-scraping.html',\n",
       " 'http://pyvideo.org/pydata-san-francisco-2016/how-soon-is-now-extracting-publication-dates-with-machine-learning.html',\n",
       " 'http://pyvideo.org/pygotham-2016/introduction-to-web-scraping-using-scrapy.html',\n",
       " 'http://pyvideo.org/pygotham-2016/webscraping-by-example-an-introduction-to-beautifulsoup.html',\n",
       " 'http://pyvideo.org/chipy/scraping-with-python.html']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the links to video pages \n",
    "talks = soup.select('article > section > h4 > a')\n",
    "talk_links = ['http://pyvideo.org' + talk['href'] \n",
    "              for talk in talks]\n",
    "talk_links[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Get a page for one of the talks \n",
    "resp = requests.get(talk_links[0])\n",
    "resp.raise_for_status()\n",
    "soup = BeautifulSoup(resp.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Get the talk's title\n",
    "titles = [title.contents[0].strip() \n",
    "          for title in soup.select('.entry-title > a')]\n",
    "title = titles[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Marie Whittaker']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the speaker's name \n",
    "names = [elem.contents[0] \n",
    "         for elem in soup.select('.url')]\n",
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a href=\"http://pyvideo.org/events/pydata-dc-2016.html\">PyData DC 2016</a>,\n",
       " <a href=\"https://www.youtube.com/watch?v=kc676iLvib8\" rel=\"external\">YouTube</a>,\n",
       " <a href=\"http://pyvideo.org/tag/data/\">Data</a>,\n",
       " <a href=\"http://pyvideo.org/tag/scraping/\">scraping</a>,\n",
       " <a href=\"http://pyvideo.org/tag/web/\">web</a>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the talk details\n",
    "details = [detail \n",
    "           for detail in soup.select('.details-content > ul > li > a')]\n",
    "details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Get the YouTube link\n",
    "links = [detail['href'] \n",
    "         for detail in details \n",
    "         if 'youtube.com' in detail['href']]\n",
    "link = links[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data', 'scraping', 'web']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the subject tags\n",
    "tags = [link.contents[0].lower() \n",
    "        for link in details \n",
    "        if 'tag' in link['href']]\n",
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyData DC 2016\n",
      "\n",
      "Distilling a world of data down to a few key indicators can be an effective way of keeping an audience informed, and this concept is at the heart of a good dashboard. This talk will cover a few methods of scraping and reshaping open data for dashboard visualization, to automate the boring stuff so you have more time and energy to focus on the analysis and content.\n",
      "\n",
      "This talk will cover a basic scenario of curating open data into visualizations for an audience. The main goal is to automate data scraping/downloading and reshaping. I use python to automate data gathering, and Tableau and D3 as visualization tools -- but the process can be applied to numerous analytical/visualization suites.\n",
      "\n",
      "I'll discuss situations where a dashboard makes sense (and when one doesn't). I will make a case also that automation makes for a more seamless data gathering and updating process, but not always for smarter data analysis.\n",
      "\n",
      "Some python packages I'll cover for web scraping and downloading/reshaping open data include: openpyxl, pandas, xlsxwriter, and BeautifulSoup. I'll also touch on APIs.\n"
     ]
    }
   ],
   "source": [
    "# Get the description\n",
    "paragraphs = soup.select('.entry-content > p')\n",
    "description = '\\n\\n'.join([p.contents[0] for p in paragraphs])\n",
    "print(description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##  Saving the Data\n",
    "\n",
    "* Use namedtuple to capture information about PyVideo talk\n",
    "\n",
    "* What do we need to capture\n",
    "    - Talk title (string)\n",
    "    - Name of presenter(s) (list)\n",
    "    - Description (string)\n",
    "    - Tags (list)\n",
    "    - Link to video (string)\n",
    "    \n",
    "* Let's use JSON \n",
    "    - Awesome for persisting structured data \n",
    "    - Excellent data exchange format\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title=Open Data Dashboards & Python Web Scraping\n",
      "names=['Marie Whittaker']\n",
      "tags=['data', 'scraping', 'web']\n",
      "link=https://www.youtube.com/watch?v=kc676iLvib8\n"
     ]
    }
   ],
   "source": [
    "# Storing related data in a namedtuple\n",
    "from collections import namedtuple\n",
    "\n",
    "# create the structure\n",
    "pyvideo = namedtuple('pyvideo', 'title names description tags link')\n",
    "\n",
    "# create an instance\n",
    "talk_data = pyvideo(title=title, names=names, tags=tags, link=link, description=description)\n",
    "\n",
    "# access the data and display it as a formatted string\n",
    "fmt = 'title={}\\nnames={}\\ntags={}\\nlink={}'\n",
    "print(fmt.format(talk_data.title, talk_data.names, talk_data.tags, talk_data.link))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Writing the namedtuple to disk as JSON\n",
    "import json\n",
    "\n",
    "# Write to file\n",
    "with open('a_video.json', 'w') as fout:\n",
    "    # Note: preserve keys by saving a dictionary, not a list\n",
    "    json.dump(talk_data._asdict(), fout)\n",
    "    \n",
    "# Read from file\n",
    "with open('a_video.json', 'r') as fin:\n",
    "    restored_talk_data = json.load(fin)\n",
    "\n",
    "# Deserialize -- Note: pass as keyword arguments using **\n",
    "restored_talk_data = pyvideo(**restored_talk_data)\n",
    "\n",
    "# Compare semantically\n",
    "talk_data == restored_talk_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Writing web scraping code has advantages\n",
    "\n",
    "* Excellent source of data \n",
    "\n",
    "* Scarce data are more valuable data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ... and presents some problems too\n",
    "\n",
    "* Your code breaks when the site is updated\n",
    "\n",
    "* Read the terms and conditions, otherwise ...\n",
    "\n",
    "* Getting blocked, banned, sued ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Acknowledgements\n",
    "\n",
    "* [Galvanize](https://www.galvanize.com/pick-a-location?page=%2F) for hosting [Denver Data Science Day 2017](http://denverdatascienceday.com/) and all the work they did to make it happen.\n",
    "\n",
    "* Bob Mickus and all the volunteers from [PyData Denver](https://www.meetup.com/PyData-Denver/) who helped organize this event [Denver Data Science Day](http://denverdatascienceday.com/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Resources\n",
    "1. [Automate the Boring Stuff](https://automatetheboringstuff.com/), [Chapter 11 — Web Scraping](https://automatetheboringstuff.com/chapter11/) by Al Sweigart (Free PDF version online).  Takes you through topics step-by-step, includes using Selenium to fill out forms and simulate mouse clicks. \n",
    "\n",
    "1. [RealPython Blog -- Web Scraping With Scrapy and MongoDB](https://realpython.com/blog/python/web-scraping-with-scrapy-and-mongodb/) by Micheal Herman. Scrapy is a Python package that makes scraping code easier to maintain. \n",
    "\n",
    "1. [Talk Python to Me Podcast -- Web scraping at scale with Scrapy and ScrapingHub](https://talkpython.fm/episodes/show/50/web-scraping-at-scale-with-scrapy-and-scrapinghub). Web scraping as a Service from the author of Scrapy.\n",
    "\n",
    "1. [PyVideo.org](http://pyvideo.org/)— Comprehensive catalog of videos of over 8000 of Python related presentations. Talks on scraping web pages can be found on the [Scraping page](http://pyvideo.org/tag/scraping/). \n",
    "\n",
    "1. [Web Scraping with Python: Collecting Data from the Modern Web](https://www.amazon.com/Web-Scraping-Python-Collecting-Modern/dp/1491910291) by Ryan Mitchell.  This 4.5 star book on Amazon covers scraping topics in depth.\n",
    "\n",
    "1. [Awesome Python](https://awesome-python.com/) -- PyPI has over 100,000 packages.  Awesome Python is a curated list of the best, see their recommended web scraping packages [here](https://awesome-python.com/#web-crawling).\n",
    "\n",
    "1. Practice Sites\n",
    "    * [Books to Scrape](http://books.toscrape.com/)\n",
    "    * [Quotes to Scrape](http://quotes.toscrape.com/) "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
